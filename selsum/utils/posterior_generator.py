import torch.nn as nn
import torch
from typing import Dict, List, Optional
from torch import Tensor
from fairseq.models.fairseq_encoder import EncoderOut
from torch.nn.functional import softmax
import numpy as np


class PosteriorGenerator(nn.Module):
    """A simple sequence sampler for the posterior. Performs sampling or
     greedy selection over the available vocabulary elements.

    Retains gradient for the produced probabilities.
    """

    incremental_states = Dict[str, Dict[str, Optional[Tensor]]]

    def __init__(self, model, bos_idx, pad_idx, temperature=1., greedy=False):
        super().__init__()
        self.model = model
        self.bos_idx = bos_idx
        self.pad_idx = pad_idx
        self.temperature = temperature
        self.greedy = greedy
        self.incremental_state = torch.jit.annotate(Dict[str,
                                                         Dict[str, Optional[Tensor]]], {})

    @torch.no_grad()
    def forward(self, sample: Dict[str, Dict[str, Tensor]], max_seq_len):
        self.reset_incremental_state()
        return self._generate(sample=sample, max_seq_len=max_seq_len)

    def _generate(self, sample, max_seq_len):
        """Generates a sequence of document indxs.

        Args:
            sample (dict): sample for which to generate a sequence.
            max_seq_len (int): the maximum length of the samples sequence.

        Returns:
            doc_indxs_coll (LongTensor): generated indxs of documents by the
                distribution.
                (batch_size, max_seq_len)
            prob_coll (FloatTensor): probabilities generated by the distribution
                at each step.
                (batch_size, max_seq_len, vocab_size)
        """
        encoder_outs = self.forward_encoder(sample['net_input'])

        ndocs = (~sample['net_input']['feats_mask']).sum(-1)
        bsz, sln, dim = encoder_outs.encoder_out.shape
        device = encoder_outs.encoder_out.device

        if max_seq_len > sln:
            raise ValueError("Maximum sequence length can't be larger than the "
                             "number of available documents.")

        # initializing the container
        sel_doc_indx_coll = torch.zeros((bsz, max_seq_len + 1),
                                        dtype=torch.long, device=device)\
            .fill_(self.bos_idx)
        sel_doc_prob_coll = torch.zeros((bsz, max_seq_len + 1), device=device)

        for step in range(max_seq_len):
            probs = self.forward_decoder(sel_doc_indx_coll[:, step].unsqueeze(-1),
                                         encoder_out=encoder_outs,
                                         temperature=self.temperature).squeeze(1)

            # dealing with padded entries by assigning all probability to the
            # padding document
            probs[ndocs <= step] = 0.
            probs[ndocs <= step, self.pad_idx] = 1.

            if self.greedy:
                new_doc_indx = torch.argmax(probs, dim=-1)
            else:
                new_doc_indx = sample_doc_indx(probs)

            # collecting
            sel_doc_indx_coll[:, step + 1] = new_doc_indx
            sel_doc_prob_coll[:, step + 1] = probs[torch.arange(bsz), new_doc_indx]

        sel_doc_indx_coll = sel_doc_indx_coll[:, 1:]
        sel_doc_prob_coll = sel_doc_prob_coll[:, 1:]

        return sel_doc_indx_coll, sel_doc_prob_coll

    def reset_incremental_state(self):
        self.incremental_state = torch.jit.annotate(Dict[str,
                                                         Dict[str, Optional[Tensor]]],
                                                    {})
        return

    @torch.jit.export
    def forward_encoder(self, net_input: Dict[str, Tensor]):
        return self.model.encoder.forward_torchscript(net_input)

    @torch.jit.export
    def forward_decoder(self, prev_sel_indxs, encoder_out: List[EncoderOut],
                        temperature: float = 1.0):
        scores = self.model.decoder.forward(prev_sel_indxs=prev_sel_indxs,
                                            encoder_out=encoder_out,
                                            incremental_state=self.incremental_state)
        scores[:, :, self.pad_idx] = - np.inf
        probs = softmax(scores.div_(temperature), dim=-1)
        return probs


def sample_doc_indx(probs):
    """There seems to be a bug on the level of PyTorch that events with zero
    probabilities are selected.
    https://github.com/pytorch/pytorch/issues/4858
    https://github.com/pytorch/pytorch/issues/13867

    Mitigates this issue by re-sampling.
    """
    bsz = probs.size(0)
    new_doc_indx = torch.multinomial(probs, num_samples=1).squeeze(-1)
    sel_probs = torch.arange(bsz)
    if (probs[sel_probs, new_doc_indx] == 0.).any():
        return sample_doc_indx(probs)
    return new_doc_indx
